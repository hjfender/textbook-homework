{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on a universal blueprint that can be used to attack and solve any machine-learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define the Problem and Assemble a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the problem:\n",
    "- What will your input data be? What are you trying to predict?\n",
    "- What type of problem are you facing? Is it binary classification? Multiclass classification? Scalar regression? Vector regressions? Multiclass, multilable classification? Something else? Identifying the problem will guide the choice of model architecture, loss fuction, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't move on until the above is complete. Be aware of that you are assuming the following:\n",
    "- Your outputs can be predicted given your inputes\n",
    "- Your available data is sufficiently informative to learn the relationships between inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Choosing a measure of Success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it appropriate to measure:\n",
    "- Accuracy?\n",
    "- Precision and recall?\n",
    "- Customer-retention rate?\n",
    "\n",
    "The metric for success will guide the choice of a loss function: what your model will optimize. It should align with your higher-level goals.\n",
    "\n",
    "- For balanced-classification problems, accuracy and are under the receiver operating characteristic curve (ROC AUC) are common metrics.\n",
    "- For ranking problems or multilabel classification, mean average precision\n",
    "\n",
    "Not uncommon to define your own metric to measure success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Deciding on an Evaluation Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose one of the 3 common evaluation proceedures:\n",
    "- Hold-out validation\n",
    "- K-fold cross-validation\n",
    "- Iterated K-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preparing your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data should be formatted as tensors\n",
    "- Values taken by these tensors should usually be scaled to small values\n",
    "- If the data is heterogeneous, normalize it\n",
    "- Do some feature engineering, especially for small-data problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Developing a model that does better than a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal at this stage is to achieve *statistical power* (beat a dumb baseline). It isn't always possible to do this. If you can't beat the random baseline after trying multiple reasonable architectures, it may be that the answer to the question you're asking isn't present in the input data. In this case, it's back to the drawing board.\n",
    "\n",
    "If things go well, you'll need to make 3 key choices to build a first working model:\n",
    "- *Last-layer activation*: This establishes useful constraints on the network's output.\n",
    "- *Loss function*: This should match the type of problem you're trying to solve.\n",
    "- *Optimization configuration*: What optimizer will you use? What will its learning rate be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table can you help you choose last-layer activation and loss fuction for a few common problem types:\n",
    "| Problem Type | Last-layer activation | Loss function |\n",
    "| ------------ | --------------------- | ------------- |\n",
    "| Binary classification | `sigmoid` | `binary_crossentropy` |\n",
    "| Multiclass, single-label classification | `softmax` | `categorical_crossentropy` |\n",
    "| Multiclass, multilabel classification | `sigmoid` | `binary_crossentropy` |\n",
    "| Regression to arbitrary values | None | `mse` |\n",
    "| Regression to values between 0 and 1 | `sigmoid` | `mse` or `binary_crossentropy` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scaling Up: developing a model that overfits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider:\n",
    "- Is your model sufficiently powerful?\n",
    "- Does it have enough layers and parameters to properly model the problem?\n",
    "\n",
    "To figure out where the border between underfitting and overfitting is, you must first cross it.\n",
    "\n",
    "To figure out how big a model you'll need, you must develop a model that overfits.\n",
    "- add layers\n",
    "- make the layers bigger\n",
    "- train for more epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Regularizing your model and turning your hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will take most of your time. You'll repeatedly modify your model, train it, evaluate it, modify it again, repeat..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some things you should try at this stage:\n",
    "- add dropout\n",
    "- try different architectures: add or remove layers\n",
    "- add L1 and/or L2 regularization\n",
    "- try different hyperparameters\n",
    "- iterate on feature engineering: add new features, or remove features that don't seem to be informative"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

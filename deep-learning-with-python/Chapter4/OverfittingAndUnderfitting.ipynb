{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, model performance on the hold-out set peaks after a few epochs and then degrades due to **overfitting**.\n",
    "\n",
    "The fundamental issue in machine learning is the tension between *optimization* (getting the best possible performance on the training data) and *generalization* (best performance on data the model hasn't seen before). At the beginning of training, optimization and generalization are correlated. At this point, the model is said to be **underfit** because the network hasn't yet learned all the relevant patterns in the data.\n",
    "\n",
    "The best way to prevent a model from learning misleading or irrelevant patterns is to *get more training data*. When that isn't possible, the next-best solution is to *modulate the quantity of information that your model is allowed to store or to add constraints on what information it's allowed to store*. (If a model can only memorize a small number of patterns, the optimization process will force it to focus on the most prominent patterns.) The process of fighting overfitting this way is called **regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some of the most common regularization techniques..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the Network's Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of a model is the number of learnable parameters in the model. This number is often referred to as the model's *capacity*. A model with more parameters has more *memorization capacity* and can easily begin a approximate a mapping between training inputs and training outputs (which has no predictive value). So the simplest way to prevent overfitting would be to reduce its size (capacity).\n",
    "\n",
    "At the same time, don't reduce the size of the model too much or else it will always underfit on the data (won't be able to memorize complex enough patterns to be useful).\n",
    "\n",
    "There is no formula for finding the right balance and this technique requires the art of feature engineering to be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Weight Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Occam's razor**: given two explanations for something, the explanation most likely to be correct is the simplest one (the one that makes the fewest assumptions)\n",
    "\n",
    "This idea also applies to the models learned by neural networks: simpler models are less likely to overfit than complex ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **simple model** means a model where the distribution of parameter values has less entropy. Thus a common way to mitigate overfitting is to put constraints on the complexity of a network by forcing its weights to take only small values, which makes the distribution of weight values more *regular*. This is called **weight regularization** and it's done by adding to the loss function of the network a *cost* associated with having large weights.\n",
    "- **L1 regularization**: The cost added is proportional to the *absolute value of the weight coefficients*\n",
    "- **L2 regularization**: The cost addes is proportional to the *square of the value of the weight coefficients* (also referred to as *weight decay* in the context of neural networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers, models, layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(10000,)))\n",
    "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropout** is a very popular and effective regularization technique. Dropout, applied to a layer, consists of randomly *dropping out* a number of output features of the layer during training (sets the value of those features to 0). The *dropout rate* is the fraction of the features that are zeroed out. At test time, no units are dropped out; instead, the layer's output values are scaled down by a factor equal to the dropout rate (to balance for the fact that more units are active than a training time)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textbook-homework-S1kjZFOz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

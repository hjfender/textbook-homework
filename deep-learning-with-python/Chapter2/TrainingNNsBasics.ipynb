{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple NN Layer Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Neural layers transform input data $x$ according to some transform like\n",
    "\n",
    "$f(W, x) = relu(W \\cdot x + b)$\n",
    "\n",
    "$W$ and $b$ are the *weights* or *training parameters* of the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially `W` is populated with small random values (*random initialization*) and are adjusted iteratively through a *training* process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical *training loop* follows these steps:\n",
    "1. Draw a batch of training samples $x$ and corresponding targets $y$\n",
    "2. Run the network on $x$ (a step called *forward pass*) to obtain predictions $y_{pred}$\n",
    "3. Compute the loss of the network on the batch, a measure of the mismatch between $y_{pred}$ and $y$\n",
    "4. Update all weights of the network in a way that slightly reduces the loss on the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform step 4, take advantage of the fact that layer transformations are *differentiable* and compute the *gradient* with respect to `W` of the *loss*. Move the weights in the opposite direction of the gradient to effectively reduce the *loss*.\n",
    "\n",
    "If $x$ and $y$ are fixed, i.e. consider just the batch at hand, we have\n",
    " \n",
    "$l(W) = loss(f(W, x), y)$\n",
    "\n",
    "and\n",
    "\n",
    "$W_1 = W_0 - s * \\nabla l(W_0)$\n",
    "\n",
    "where $s$ is a small *step* scaling factor.\n",
    "\n",
    "Computing $\\nabla l(W)$ is called *backward pass*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the theoretical minimum of the differentiable function $l$, i.e. when $\\nabla l(W) = 0$. In practice we do this by taking the small steps described above in step 4. This ios called *mini-batch stochastic gradient descent*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the concept of *momentum* to choose the right step size and avoid getting stuck at local minimum or taking too long to converge.\n",
    "\n",
    "```\n",
    "past_velocity = 0\n",
    "momentum = 0.1\n",
    "while loss > 0.01:\n",
    "    W, loss, gradient = get_current_parameters()\n",
    "    velocity = past_velocity * momentum - learning_rate * gradient\n",
    "    W = W + momentum * velocity - learning_rate * gradient\n",
    "    past_velocity = velocity\n",
    "    update_parameter(W)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A network $f$ may be composed of 3 tensor operations with weight matrices $W_1, W_2, W_3$\n",
    "\n",
    "$f(W_1, W_2, W_3) = f_1(W_1, f_2(W_2, f_3(W_3)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chain rule tells us that these chains of functions are derived via\n",
    "$f(g(x)) = f'(g(x)) * g'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This rule implies we can start at the final loss value and work backward from the top layers to the bottom layers, applying the chain rule to compute the contribution that each parameter had in the loss value. This is called *backpropagation* or *reverse-mode differentiation*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textbook-homework-S1kjZFOz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
